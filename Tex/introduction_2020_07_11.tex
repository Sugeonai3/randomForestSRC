\RequirePackage[T1]{fontenc}
\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.200] pzcmi7t}{}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
%\documentclass[twoside,12pt]{article}
%\documentclass[leqno,twoside,11pt,english]{article}
\documentclass[twoside,11pt,english]{article}
\usepackage{times,amsmath,amsfonts,amsthm,amssymb,eucal}
\usepackage{bbold}
\usepackage{graphicx,ifthen}
\usepackage{wrapfig}             
\usepackage{latexsym}
\usepackage{color}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}

%\usepackage{subfigure}
\usepackage{rotating}
\usepackage{subcaption}

\usepackage{setspace}

%\usepackage[square, numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[round]{natbib}

%%%%%%%%%%%%%%% always put hyperref last%%%%%%%%%%%%%%%%%%%

\usepackage[hyperfootnotes=false]{hyperref}
\hypersetup{
  colorlinks = true,%
  linkcolor = red,%
  citecolor = blue,%
 linkbordercolor = white,%
  hyperfootnotes = false%
}


\setlength{\textwidth}{5.5in}\setlength{\textheight}{8.75in}
\setlength{\evensidemargin}{0.5in}\setlength{\oddsidemargin}{0.5in}
\setlength{\topmargin}{-0.15in}\setlength{\headsep}{0.25in}

%\setlength{\textwidth}{6.5in}\setlength{\textheight}{9.0in}
%\setlength{\evensidemargin}{0.0in}\setlength{\oddsidemargin}{0.0in}
%\setlength{\topmargin}{-0.25in}\setlength{\headsep}{0.25in}
%\setlength{\footskip}{-0.5in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  over-ride default theorem style
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\usepackage{thmtools}
\declaretheoremstyle[headfont=\SMC]{smchead}
\declaretheoremstyle[
  spaceabove=6pt, spacebelow=6pt,
  headfont=\SMC,
  notefont=\mdseries, notebraces={(}{)},
  bodyfont=\normalfont,
  %qed=\qedsymbol,
  %postheadspace=1em,  
]{mystyle}
%\declaretheorem[style=mystyle]{styledtheorem}
\declaretheorem[style=mystyle]{remark}
\declaretheorem[style=mystyle]{example}
%\declaretheorem[style=mystyle]{definition}
%\declaretheorem[style=mystyle]{assumption}


%%%%%%%%% Old school Lt and Gt  %%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareFontFamily{U}{matha}{\hyphenchar\font45}
\DeclareFontShape{U}{matha}{m}{n}{
      <5> <6> <7> <8> <9> <10> gen * matha
      <10.95> matha10 <12> <14.4> <17.28> <20.74> <24.88> matha12
      }{}
\DeclareSymbolFont{matha}{U}{matha}{m}{n}

\DeclareMathSymbol{\Lt}{3}{matha}{"CE}
\DeclareMathSymbol{\Gt}{3}{matha}{"CF}



\begin{document}
\DeclareGraphicsExtensions{.gif,.pdf,.png,.jpg,.tiff,.jpf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  what follows are a collection of aliases that will be useful for
%%  almost all your documents
%%
%%  below it are document specific aliases
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Delimters 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\({\left(}
\def\){\right)}
\def\[{\left[}
\def\]{\right]}
%\def\{{\left\{}
%\def\}{\right\{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%Greek
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\a{\alpha}\def\b{\beta}\def\d{\delta}\def\D{\Delta}
\def\e{\varepsilon}\def\g{\gamma}\def\G{\Gamma}\def\k{\kappa}
\def\l{\lambda}\def\L{\Lambda}\def\m{\mu}\def\p{\phi}\def\P{\Phi}
\def\r{\rho}\def\s{\sigma}\def\t{\theta}\def\T{\Theta}
\def\ta{\tau}\def\z{\zeta}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%Ralph Smith Formal Script
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\RSF{\mathscr}
\def\aa{{\RSF A}}\def\bb{{\RSF B}}\def\cc{{\RSF C}}
\def\dd{{\RSF D}}\def\ee{{\RSF E}}\def\ff{{\RSF F}}
\def\gg{{\RSF G}}
\def\hh{{\RSF H}}\def\ii{{\RSF I}}\def\jj{{\RSF J}}\def\kk{{\RSF K}}
%\def\ll{{\RSF L}} 
\def\mm{{\RSF M}}\def\nn{{\RSF N}}\def\oo{{\RSF O}}
\def\pp{{\RSF P}}
\def\qq{{\RSF Q}}\def\rr{{\RSF R}}\def\ss{{\RSF S}}
\def\tt{{\RSF T}} \def\uu{{\RSF U}}\def\vv{{\RSF V}}\def\ww{{\RSF W}}
\def\xx{{\RSF X}} \def\yy{{\RSF Y}}\def\zz{{\RSF Z}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%Fonts
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\font\Caps=cmcsc10 \font\BigCaps=cmcsc9 scaled \magstep 1
\font\BigSlant=cmsl12    
\font\proclaimfont=cmbx9 scaled \magstep 1
\def\BigHeading{\bfseries\Large}\def\MediumHeading{\bfseries\large}
\def\smc{\Caps}\def\SMC{\BigCaps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%Skips
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\lbk{\linebreak}
\newdimen\bigindent
\newdimen\smallindent
\bigindent=30pt \smallindent=5pt
\def\quoteindent{\advance\leftskip by\bigindent\advance\rightskip
                 by\bigindent}
\newskip\proclaimskipamount
\proclaimskipamount=12pt  plus1pt minus1pt
\def\proclaimskip{%
  \par\ifdim\lastskip<\proclaimskipamount
  \removelastskip\vskip\proclaimskipamount\fi}
\let\demoskip=\proclaimskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Sections, Theorems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\Demo#1{\par\ifdim\lastskip<\proclaimskipamount
            \removelastskip\proclaimskip\fi
        \sl#1. \hskip\smallindent\rm}
\def\EndDemo{\par\demoskip}
\def\DemoSection#1{\par\ifdim\lastskip<\proclaimskipamount
             \removelastskip\proclaimskip\fi
             #1\hskip\smallindent\rm}
\def\Section#1{\stepcounter{section}
    \DemoSection{{\bfseries\large\thesection.\hskip\smallindent#1.}}}
\def\Subsection#1{\stepcounter{subsection}
    \DemoSection{\it\normalsize\normalsize\thesubsection.\hskip\smallindent#1.}}
\def\SubSubsection#1{\stepcounter{subsubsection}
    \DemoSection{\it\normalsize\thesubsubsection.\hskip\smallindent#1.}}
\def\Quote{\begin{quotation}\normalfont\small\noindent}
\def\EndQuote{\end{quotation}\rm}
\def\Message{\begin{quotation}\it\noindent}
\def\EndMessage{\end{quotation}\rm}



\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
%\newtheorem{remark}{Remark}
%\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{strategy}{Strategy}

\def\Theorem{\begin{theorem}\sl}
\def\EndTheorem{\end{theorem}}
\def\Lemma{\begin{lemma}\sl}
\def\EndLemma{\end{lemma}}
\def\Corollary{\begin{corollary}\sl}
\def\EndCorollary{\end{corollary}}
\def\Remark{\begin{remark}\rm}
\def\EndRemark{\end{remark}}
\def\Example{\begin{example}\rm}
\def\EndExample{\end{example}}
\def\Definition{\begin{definition}\sl}
\def\EndDefinition{\end{definition}}
\def\Assumption{\begin{assumption}\sl}
\def\EndAssumption{\end{assumption}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%Lists, equations 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\bct{\begin{center}}
\def\ect{\end{center}}
\def\Array{\begin{eqnarray*}}
\def\EndArray{\end{eqnarray*}}
\def\Enumerate{\begin{enumerate}}
\def\EndEnumerate{\end{enumerate}}
\def\Eq{\begin{equation}}
\def\EndEq{\end{equation}}
\def\EqArray{\begin{eqnarray}}
\def\EndEqArray{\end{eqnarray}}
\def\Itemize{\begin{itemize}}
\def\EndItemize{\end{itemize}}
\def\mref#1{(\ref{#1})}
\def\qt#1{\qquad\text{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%Tables, Alignment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\Tabular{\begin{tabular}}
\def\EndTabular{\end{tabular}}
\def\FlushLeft{\begin{flushleft}}
\def\EndFlushLeft{\end{flushleft}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%Color
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\green}{\textcolor{green}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%argmin argmax
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\argmax{\mathop{\rm argmax}}% argmax; indices appear below word
\def\argmin{\mathop{\rm argmin}}% argmin; indices appear below word
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%probability 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\as{\stackrel{\text{a.s.}}\rightarrow}
\def\dist{\stackrel{\mathrm{d}}{\leadsto}}
\def\Dist{\stackrel{d}\rightarrow}
\def\equalas{\,\smash{\mathop{=}\limits^{\text{a.s}}}\,\,\,}
\def\equald{\,\smash{\mathop{=}\limits^{\dd}}\,}
\def\prob{\stackrel{\text{p}}\rightarrow}
\def\wp1{\stackrel{\text{with probability one}}\rightarrow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%closure/interior sets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*\interior[1]{\mathring{#1}}
\newcommand*\closure[1]{\bar{#1}}


%%%%%%%%%% R-code %%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\rcode}[1]{{\ttfamily #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Document specific definitions 
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\avgCov{\overline{\text{Cov}}}
\def\Bias{\text{Bias}}
\def\Chat{\hat{C}}
\def\Cov{{\text{Cov}}}
\def\eps{\varepsilon}
%\def\fehat{\hat{f}_{\text{e}}^{(M)}}
\def\fehat{\hat{f}_{\text{ens}}}
\def\fhat{\hat{f}}
\def\fbar{\overline{f}}
\def\muhat{\hat{\mu}}
\def\oob{\text{oob}}
\def\p{\varphi}
\def\phat{\hat{\p}}
\def\P{{\bf P}}
\def\PE{\text{Err}}
\def\PEhat{\hat{\PE}}
\def\PEloo{\text{Err}_{\text{loo}}}
\def\phihat{\hat{\phi}}
\def\psihat{\hat{\psi}}
\def\R{{\bf R}}
\def\S{{\bm\Sigma}}
\def\ss{{\bf s}}
\def\SS{{\bf S}}
\def\Shat{\hat{S}}
\def\T{{\Theta}}
\def\tr{\text{tr}}
\def\U{{\bf U}}
\def\v{{\bf v}}
\def\V{{\bf V}}
\def\Var{\text{Var}}
\def\W{{\bf W}}
\def\x{{\bf x}}
\def\X{{\bf X}}
\def\xlbar{\overline{x}}
\def\Xlbar{\overline{X}}
\def\xbar{\overline{\x}}
\def\Xbar{\overline{\X}}
\def\y{{\bf y}}
\def\Y{{\bf Y}}
\def\yhat{\hat{y}}
\def\Yhat{\hat{Y}}
\def\ybar{\overline{y}}
\def\Ybar{\overline{Y}}
\def\Z{{\bf Z}}
\def\z{{\bf z}}

\def\LL{\mathpzc{L}}

\def\E{\mathbb{E}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\RR{\mathbb{R}}
\def\QQ{\mathbb{Q}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Document begins here 
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\Report{An Introduction to \rcode{randomForestSRC}}
\def\Author{Hemant Ishwaran and Udaya B. Kogalur}\pagestyle{myheadings}\markboth{\Author}{\Report}
\thispagestyle{empty}

\bct{\Huge\bf \rcode{randomForestSRC}\vskip10pt}
    {\Large\sl Hemant Ishwaran and Udaya B. Kogalur}
    \\[10pt]
    \rm\today 
\ect



\section*{Introduction}

\rcode{randomForestSRC} is a CRAN compliant R-package that fits random
forests~\citep{Breiman2001} to a wide variety of settings.  The
package uses fast OpenMP parallel computing and grows forests for
regression, classification, survival analysis, competing risks,
multivariate, unsupervised, quantile regression, and class imbalanced
q-classification problems.  The package is constantly evolving and it
is anticipated that over time many new kinds of applications, forests
and tree constructions will be added to it.

This package was developed by Hemant Ishwaran and Udaya. B. Kogalur and is the
offspring of the original parent package \rcode{randomSurvivalForest}
developed by the same authors to fit random forests to survival data.
At that time, Breiman's random forest (RF) was only
available for regression and classification.  Random survival
forests~\citep{Ishwaran2008} (RSF) was introduced to extend RF to the
setting of right-censored survival data. The implementation of RSF was
designed to follow the same general principles as RF.

As we can see, there are many interesting applications of random
forests, so then what exactly is a random forest?  Basically, RF is is
a tree-ensemble method for learning.  The basic idea is that
constructing ensembles by the process of averaging simple functions,
called base learners, such as trees, substantially improves prediction
performance.  RF builds on this concept by injecting further
randomization into the learning process.  Specifically, randomization
is introduced in two forms. First, a randomly drawn bootstrap sample
of the data is used to grow a tree (actually there is nothing special
about the bootstrap, and other types of sampling can be used).
Second, during the grow stage at each node of the tree, a randomly
selected subset of variables is chosen as candidates for splitting
(this is called random feature selection).  The purpose of this
two-step randomization is to decorrelate trees, which reduces the
variance due to the property of bagging~\citep{Breiman1996}.
Furthermore, RF trees are typically grown very deeply; in fact,
Breiman's original RF classifier called for growing a classification
tree to purity (one observation per terminal node).  The use of deep
trees, a bias reduction technique, when combined with reduced variance
due to averaging and randomization, enables RF to approximate rich
classes of functions while maintaining low generalization error.


\section*{Why Do Ensembles Work?  A More Technical Explanation}

To make the last part more clear we provide a more detailed
explanation for why ensembles work in general.  We explain why
averaging simple base-learners leads to good prediction performance
for methods like RF.  For simplicity, we consider the regression case.
Let $\{\p_1,\ldots,\p_B\}$ be a collection of learners where each
learner $\p_b:\xx\rightarrow\RR$ is trained on the learning data
$\LL=\{(\X_1,Y_1), \ldots, (\X_n,Y_n)\}$ to estimate a target
function $f$.  Here  $Y$ is the scalar outcome and $\X\in\xx$ is the
feature vector.  It is assumed that each learner is trained separately
from one another.  Because the learners are trained on the same data
they cannot be independent, however we will assume that they share the
same common distribution.

Define the ensemble estimator as the averaged value of the learners
$$
\fehat(\x)=\frac{1}{B}\sum_{b=1}^B \p_b(\x).
$$
Let $(\X,\Y)$ be an independent test data point with the same
distribution as the learning data.  The generalization error for an
estimator $\fhat$ is
$$
\PE(\fhat)= \E_\LL\E_{\X,Y}\Bigl[Y-\fhat(\X)\Bigr]^2.
$$
Assuming a regression model
$Y=f(\X)+\eps$ where $\Var(\eps)=\s^2$,
using a bias-variance decomposition, we have
$$
\PE(\fhat) = \s^2 + \E_\X \left\{\Bias\{\fhat\,|\,\X\}^2
  + \Var\{\fhat\,|\,\X\}\right\}
$$
where the two terms on the right are
the conditional bias and conditional variance for $\fhat$.
%defined by
%\Array
%\Bias\{\fhat\,|\,\X\}
%&=& \E_{\LL}\left\{\fhat(\X)\right\} - f(\X)\\
%\Var\{\fhat\,|\,\X\}
%&=& \E_{\LL}\left\{\(\fhat(\X)-\E_{\LL}[\fhat(\X)]\)^2\right\}.
%\EndArray
Using this notation, we can establish the following
result~\citep{ueda1996generalization}.

\Theorem\label{T:randomForest}
If $\{\p, \p_1,\ldots,\p_B\}$ are identically distributed
learners constructed from $\LL$, then
$$
\PE(\fehat)
=
\s^2 + 
\E_\X \left\{\Bias\{\p\,|\,\X\}^2
  + \frac{1}{B}\Var\{\p\,|\,\X\}
  + \(1-\frac{1}{B}\)\avgCov(\X)
  \right\}
$$
where $\avgCov(\X) = \Cov(\p_b,\p_{b'}|\X)$.
\EndTheorem

To understand Theorem~\ref{T:randomForest} keep in mind that the number of learners,
$B$, is at our discretion and can be selected as large as
we want (of course in practice this decision will be affected by
computational cost, but let's not worry about that for
now).  Therefore with a large enough collection of learners we can
expect the generalization error to closely approximate the limiting
value
$$
\lim_{B\rightarrow\infty} \PE(\fehat)
= \s^2 + \E_\X \left\{\Bias\{\p|\,\X\}^2 + \avgCov(\X) \right\}.
$$ Notice the contribution from the variance converges to zero which
is very promising.  The ideal generalization error is $\s^2$, so in
order to achieve this value, we need base learners to have bias
converging to zero. However the problem is the term $\avgCov(\X)$
which is the average covariance between any two learners.  As bias
decreases, learners must naturally become more complex, but this has
the counter effect of increasing covariance (they need to use all the
data, they need to use all variables for splitting, etc., which makes
them more correlated with each other).  We can see why RF is the way
it is.  Here the base learners are randomized trees: the randomization
is what reduces correlation.  And RF uses deep trees: a deep overfit
tree is what reduces bias.  Thus RF balances these two terms
and we can summarize the result by saying RF works because it is a
variance reduction technique for low bias learners.

\section*{Quick Overview of the Package}


%\setstretch{0.8}

\vskip 0.4in
%\bibliographystyle{myplain}
\bibliographystyle{abbrvnat}
%\bibliographystyle{agsm}
%\bibliographystyle{chicago}
%\bibliographystyle{rss}
\bibliography{vignettes}

\end{document}

